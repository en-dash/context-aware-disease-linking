\PassOptionsToPackage{utf8}{inputenc}
\documentclass{bioinfo}
\copyrightyear{2015} \pubyear{2015}

\access{Advance Access Publication Date: Day Month Year}
\appnotes{Manuscript Category}


% Notes

% Citation styles:
% \cite:    Foo et al. (2018)
% \citep:   (Foo et al., 2018)
% \citealp: Foo et al., 2018

% Equations:
% use \begin{equation}

% Tables:
% \begin{table}[!t]
% \processtable{This is table caption\label{Tab:01}}
% {\begin{tabular}{@{}llll@{}}\toprule head1 &
% head2 & head3 & head4\\\midrule
% row1 & row1 & row1 & row1\\
% row2 & row2 & row2 & row2\\
% row3 & row3 & row3 & row3\\
% row4 & row4 & row4 & row4\\\botrule
% \end{tabular}}{This is a footnote}
% \end{table}

% Figures:
% \begin{figure}[!tpb]%figure2
% \centerline{\includegraphics{fig02.eps}}
% \caption{Caption, caption.}\label{fig:02}
% \end{figure}

% Sectioning:
% use \section, \subsection, \subsubsection (numbered non-star version)

% The submission page (https://academic.oup.com/bioinformatics/pages/instructions_for_authors) says we should use:
% Introduction, System and methods, Algorithm, Implementation, Discussion, References
% The template has:
% Introduction, Approach, Methods, Discussion, Conclusion
% But recently published articles show this:
% Introduction, Materials and methods, Results, Discussion (https://doi.org/10.1093/bioinformatics/bty179, .../bty193, .../bty117)


% The template wraps parts of the Methods section in \begin{methods} --
% why? why only parts of it?

% Template uses \enlargethispage{12pt}
% Template uses \vadjust{\newpage} and \vadjust{\pagebreak} (what do they do?)


\newcommand{\url}[1]{\href{#1}{#1}}
\newcommand{\eg}{e.\,g.\ }
\newcommand{\ie}{i.\,e.\ }


\begin{document}
\firstpage{1}

\subtitle{Data and text mining}

\title[Context-aware disease linking]{Context-aware disease linking}
\author[Sample \textit{et~al}.]{Corresponding Author\,$^{\text{\sfb 1,}*}$, Co-Author\,$^{\text{\sfb 2}}$ and Co-Author\,$^{\text{\sfb 2,}*}$}
\address{$^{\text{\sf 1}}$Department, Institution, City, Post Code, Country and \\
$^{\text{\sf 2}}$Department, Institution, City, Post Code,
Country.}

\corresp{$^\ast$To whom correspondence should be addressed.}

\history{Received on XXXXX; revised on XXXXX; accepted on XXXXX}

\editor{Associate Editor: XXXXXXX}


\abstract{%
\textbf{Motivation:} This section should specifically state the scientific question within the context of the field of study.\\
\textbf{Results:} This section should summarize the scientific advance or novel results of the study, and its impact on computational biology.\\
\textbf{Availability and Implementation:} This section should state software availability if the paper focuses mainly on software development or on the implementation of an algorithm. Examples are: 'Freely available on the web at \url{http://www.example.org}'; 'Website implemented in Perl, MySQL and Apache, with all major browsers supported'; or 'Source code and binaries freely available for download at URL, implemented in C++ and supported on linux and MS Windows'. The complete address (URL) should be given. If the manuscript describes new software tools or the implementation of novel algorithms the software must be freely available to non-commercial users. Authors must also ensure that the software is available for a full TWO YEARS following publication. The editors of Bioinformatics encourage authors to make their source code available and, if possible, to provide access through an open source license (see www.opensource.org for examples).\\
\textbf{Contact:} Full email address to be given, preferably an institution email address.\\
\textbf{Supplementary information:} Links to additional figures/data available on a web site, pr reference to online-only supplementary data available at the journal's web site.
}

\maketitle

\section{Introduction}

introduce task (linking vs. NER)

discuss state of the art (nobody uses context!)



\section{Materials and methods}

\subsection{Datasets}

We evaluated the proposed system using two annotated document collections.
The ShARe corpus consists of clinical reports with annotations for diseases and disorders, which were created for the ShARe/CLEF eHealth Evaluation Lab 2013 shared task \citep{pradhan-et-al:2013:CLEF}.
The NCBI disease corpus \citep{islamaj-dogan-et-al:2014} provides disease annotations over abstracts of scientific articles.
Both collections include references to the relevant mentions in their original context and link them to identifiers of a standard knowledge base.
In both cases, a predefined division into training and test set allows comparison across different approaches.
More detailed statistics on the datasets are given in Table~\ref{tab:datasets}.

The ShARe corpus contains de-identified clinical reports from US intensive care.
It is distributed as part of the MIMIC database  % TODO: reference
and is available for research purposes after acquiring a personal license.%
\footnote{Instructions for obtaining the data are given on the shared-task website, currently hosted at \url{https://sites.google.com/site/shareclefehealth/data}.}
The annotated mentions are grounded in the SNOMED CT terminology  % TODO: reference
using UMLS  % TODO: reference
identifiers (CUI).
Concepts that were not represented in SNOMED at the time of annotation were given the NIL symbol “CUI-less”.

The NCBI disease corpus comprises scientific abstracts and annotations which are freely available online.
In addition to the training and test set, a predetermined development set is provided (regarded as part of the training set in Table~\ref{tab:datasets}).
The mentions are linked to the MEDIC vocabulary  % TODO: reference
using MeSH  % TODO: reference
and OMIM  % TODO: reference
identifiers.
Unlike the ShARe corpus, the NCBI annotators were required to always provide an identifier, \ie there is no NIL label.
Furthermore, they were asked to map a mention to multiple identifiers if there was no single terminology entry that sufficiently covered the meaning of the concept in question.
As another difference, composite mentions such as “breast and ovarian cancer” are represented as a single annotation with a series of identifiers, whereas the ShARe corpus uses multiple, partially overlapping annotations (“breast [\dots] cancer” and “ovarian cancer”).

\begin{table}[!t]
\processtable{Dataset statistics.\label{tab:datasets}}
{\begin{tabular}{@{}llll@{}}\toprule
head1 & head2 & head3 & head4\\\midrule
row1 & row1 & row1 & row1\\
row2 & row2 & row2 & row2\\
row3 & row3 & row3 & row3\\
row4 & row4 & row4 & row4\\\botrule
\end{tabular}}{}
\end{table}

% TODO: describe addition of UMLS synonyms (different strategy for ShARe and NCBI, incl. usage of NIL label)


\subsection{System architecture}

We approach the entity linking task in a three-stage process:
generating candidate names (Section~\ref{ssub:cand-gen}),
ranking candidates (\ref{ssub:ranking}), and
mapping names to identifiers (\ref{ssub:id-mapping}).
In the candidate-generation stage, different strategies are used to extract a selection of synonyms from the knowledge base for each mention.
For the second stage, a CNN is trained to assign a score to each proposed mention-synonym pair.
In the last stage, the scored list is examined to pick the most likely identifier for each mention.

\subsubsection{Candidate generation}
\label{ssub:cand-gen}

The candidate generation phase is a preprocessing step for the subsequent ranking task.
It aims at keeping the ranking lists at a manageable size without missing relevant names.
A good recall value is crucial for the candidate generation, as it poses a hard upper boundary on the entire pipeline.

We implemented a candidate generator based on orthographic similarity, another one on semantic similarity, and a number of ancillary generators targeting specific cases.
For the orthographic similarity, mentions and candidates are represented as vectors of their character skip-grams (\ie n-grams with gaps),  % TODO: reference
which allows efficient computation of pair-wise cosine similarity.
The shape of the skip-grams, a similarity threshold as well as a cut-off value are experimental parameters that control the number of candidates returned for each mention.
For the semantic similarity, mentions and candidates are represented as phrase vectors which are obtained by averaging over the word vectors of pretrained embeddings.
The most similar candidates are determined through pair-wise cosine, analogously to the orthographic similarity.
The ancillary generators are concerned with specific tasks like abbreviation expansion, digit/numeral replacement, hyperonym relation, and composite mentions.
They operate in their respective niche and are triggered only in certain circumstances.

The generators provide a score for each candidate.
The cosine-based generators return a value in the range $[0,1]$, while others report $1$ for every candidate they can find.
If multiple generators produced the same candidate, their scores are aggregated; a $0$ score is substituted for each generator that did not produce the candidate in question.
For each candidate, a vector of scores is provided to the CNN, along with the candidate name.

oracle mode!

\subsubsection{Ranking}
\label{ssub:ranking}

For ranking the candidate names created in the previous step, we trained a CNN similar to the approach reported by \cite{lihaodi-et-al:2017}.
Key differences are the candidate generation process (instead of a fine-tuned rule-based system, we use a simpler and potentially more general method) and the usage of context.

\begin{figure*}[!tpb]
\centerline{\includegraphics[width=\textwidth]{img/nn-arch.pdf}}
\caption{Architecture of the CNN used for ranking candidate names.}\label{fig:sys-arch}
\end{figure*}

Figure~\ref{fig:sys-arch} shows the architecture of the CNN.
During training and prediction, each mention-candidate pair is a separate data point.
As its input, the network considers four textual units:
the current mention, the current candidate name, the context of the current mention (\ie the complete abstract/report), and a dictionary definition of the candidate concept (serving as an equivalent of the mention context).

All textual units -- mention, candidate, context, definition – share the same semantic representation in the network.
At the word level, the texts are embedded using pretrained word vectors.
We used the vectors trained on PubMed abstracts by \cite{chiu-et-al:2016:BioNLP} with a context window of 30 tokens,  % NB: Chiu et al say win-2 worked better for NER, but it's not the case here (mention in footnote?)
and applied padding to ensure fixed-length input sequences across all samples.
In the subsequent network layer, a convolution operation is applied to capture informative word n-grams.
We used different filter widths ($n \in {2,3,4}$) with 50 kernels each.
% TODO: mention tanh activation
Using 1-max pooling, the most informative feature of each kernel is extracted.
The pooled values form a feature vector carrying semantic information about the respective textual unit.

The semantic representations of all four text inputs are concatenated in the subsequent layer, together with a node each for semantic and lexical similarity as well as the scores from candidate generation.
The semantic similarity is determined by a dedicated layer for joining the semantic representations of the mention and candidate name with a weight matrix that is updated during training.
The lexical similarity is calculated as the Jaccard index  % TODO: reference
between the stemmed word tokens \citep{porter:1980} of the mention and candidate name.

The nodes of the concatenation layer are fully connected to a hidden layer of the same shape.
The scoring task is modeled as a logistic regression problem, using a single output node with sigmoid activation and binary cross-entropy loss.
% TODO: mention (and cite) optimizer?

\subsubsection{Identifier mapping}
\label{ssub:id-mapping}

Finally, the scored candidate names need to be mapped to terminology identifiers.
For each mention, the best-scoring name is determined and looked up in the terminology.
In the majority of the cases, this procedure is straight-forward, as most of the synonym names are unambiguous.  % TODO: give actual figures for both terminologies
However, if the top-ranking name maps to multiple concepts, the remainder of the ranked list is searched for disambiguation cues using the following heuristic:
\begin{enumerate}
  \item\label{enum:start} Consider the next-lower scoring candidate name $c$ and determine the set of identifiers $I_c$ to which it maps.
  \item Compute the intersection of $I_c$ with $I_t$, the identifiers of the top-scoring candidate. If $I_c \cap I_t$ is empty, repeat from Step~\ref{enum:start}.
  \item Set $I_t \leftarrow I_c \cap I_t$. If there is still ambiguity, \ie if $|I_t| > 1$, repeat from Step~\ref{enum:start}.
\end{enumerate}
If this procedure does not fully disambiguate the candidate name, we sort the identifier symbols into lexical order and emit the first one.  % in order to be deterministic
% TODO: report the number of amibiguous predictions for each dataset
% TODO: analyse how well this heuristic works

% TODO: mention threshold for NIL symbol (not used)?



\section{Results}

ShARe/CLEF: better than Li et al. and D'Souza \& Ng.

NCBI: better than Li et al., but we didn't beat TaggerOne
(where do these 88.8\% come from? -- no answer from Li et al.)



\section{Discussion}

explain and regret need for candidate generators

maybe analyse contribution of individual candidate generators (like feature ablation)?

analyse contribution of context (kernel inspection)

any differences between text genre?

ambiguity in candidate names\dots

future work: doing hierarchical classification (producing the IDs directly) would be cool. something like \cite{guo-et-al:2018} looks interesting, but how to deal with the problem that we will never have training examples for 10k (not just 100) labels?



\section*{Acknowledgements}

Submission guidelines:
Please ensure you acknowledge all sources of funding.
Details of all funding sources for the work in question should be given in a separate section entitled 'Funding'. This should appear before the 'Acknowledgements' section.

But: the template and published examples list funding later.
Nobody acknowledges source of funding.
\vspace*{-12pt}



\section*{Funding}

This work was supported by \dots

An example is given here: ‘This work was supported by the National Institutes of Health [AA123456 to C.S., BB765432 to M.H.]; and the Alcohol \& Education Research Council [hfygr667789].’
\vspace*{-12pt}



\bibliographystyle{natbib}
\bibliography{refs}


\end{document}
